---
title: WASSA and EMNLP 2018: The loneliness of the long-awaited poster session
---

Just back from presenting a paper on topic and opinion detection<sup>1</sup> at WASSA 2018 which was part of this years EMNLP, my fist time attending that, second at an ACL event and fourth overall big conference. 

Here are some thoughts.

<h2>Venue</h2>

Both the conference and the workshop took place at Square, a conference centre right in the centre of town. Considering the massive amount of attendees (c. 2,500), they didn't do to bad a job of organising everything  Coffee break snacks were not up to LREC 2018 standards, but then that was Japan.

One area where organisation broke down a little was in the poster sessions – more on that later.

The view from the 3rd floor:

![Square and Brussels](/assets/brussels.jpg)

Another thing was security, which noone seemed to be bothered about at any other conference I've been to. Here there were bag searches on every entry. I think this may be a Belgium thing.


<h2>WASSA</h2>

Despite some ongoing technical hitches with the screens, the workshop opened with an invited talk by Elen Riloff on identifying affective events and the reasons for their polarity. She suggested that around 40% of events have affective polarity, and presented work on classifying these according to a set of 'human needs', such as physiological, health & safety, financial and social needs.

Following this were the paper presentations, of which the interesting points for me were the following:

The first of many mentions this week of AllenNLP's ELMo word embeddings tool in Ilić et al.'s sarcasm detection paper.<sup>2</sup>

Saroufim et al.<sup>4</sup> pointed out that word embeddings can fail to work for sentiment analysis tasks as words may map to other words of opposite polarities. So they trained wordf embeddings with encoded sentiment, and it seemed to work.

Emily Öhman presented a gamified tool for crowd-sourced emotion anotation based on Plutchik's wheel:

![Plutchik's Wheel](/assets/Plutchik-wheel.svg)

There were several presentations on the WASSA shared task which concerned predicting emotion without obvious 'triggers' such as emoticons or words like 'happy'. Ultimately, it seems you need to work for a company with a lot of resources and data if you want to win this, or at least it helps.

Related somewhat to both sentiment and language drift, fellow IT&C at KU graduate Jan Lukes showed us that the polarity of words can change over time, and that models trained on historical data tend to perform poorly. He used [reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) of tweets from different time periods and then a fixed feature set across these to ensure that novel expressions wouldn't effect results.

The work most similar in aim to that of my project was that of Bhatia and P, who used LDA and sentiment analysis to perform 'topic specific sentiment analysis'<sup>3</sup> in an attempt to identify the political ideology of members of Congress. Why wouldn't LDA work for me again? Because LDA tends to discover neutral topics rather that policies to which one might have an opinion. Here they use party labels to represent ideology, but, in our case at least, we're not interested in predicting party affiliation – we already know it. Want we want to find out is *within*-party variation on policy topics, so the approaches used here are not really suitable.

<h3>WASSA poster session</h3>

This is where things got chaotic. Inevitably, the presentations overran, and what was already programmed to be a short session (70 mins) ended up as more like half an hour. On top of this, there was no designated WASSA area, so we hung our posters randomly in a coffee break area that already had a very home-time feel to it. As a consequence, I only spoke to a handful of people. Here's my poster and a few members of the sparse crowd:

![my poster](/assets/wassa_poster.jpg)


<h3>References</h3>
In Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis.
Association for Computational Linguistics:

1. Abercrombie, G. and Batista-Navarro, R. 2018. *Identifying Opinion-Topics and Polarity of Parliamentary Debate Motions*. 

2. Ilić, S., Marrese-Taylor, E., Balazs, J.A. and Matsuo, Y., 2018. *Deep contextualized word representations for detecting sarcasm and irony*.

3. Bhatia, S. and P, D., 2018. Topic-Specific Sentiment Analysis Can Help Identify Political Ideology.

4. Saroufim, C., Almatarky, A. and AbdelHady, M., 2018. *Language Independent Sentiment Analysis with Sentiment-Specific Word Embeddings*.

5. Öhman, E., Kajava, K., Tiedemann, J. and Honkela, T. *Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation*.

6. Lukeš, J and Søgaard, A. *Sentiment analysis under temporal shift*.
